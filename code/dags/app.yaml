apiVersion: sparkoperator.k8s.io/v1beta2
kind: SparkApplication
metadata:
  name: template
  namespace: spark
spec:
  type: Python
  pythonVersion: "3"
  mode: cluster
  image: dongoctuyen/spark:base-v3.4
  imagePullPolicy: IfNotPresent
  mainApplicationFile: local:///opt/spark/work-dir/jobs/src/main.py
  sparkVersion: 3.4.0
  arguments: ["--job", "template", "--env", "dev"]
  sparkConf:
    "spark.hadoop.fs.s3a.impl": "org.apache.hadoop.fs.s3a.S3AFileSystem"
    "spark.hadoop.fs.s3a.access.key": "admin"
    "spark.hadoop.fs.s3a.secret.key": "Qwerty@1234"
    "spark.hadoop.fs.s3a.endpoint": "http://minio-service.minio-dev.svc.cluster.local:9000"
    "spark.hadoop.fs.s3a.path.style.access": "true"
    "spark.hadoop.fs.s3a.connection.ssl.enabled": "false"
    "spark.hadoop.fs.s3a.fast.upload": "true"
    "spark.hadoop.fs.s3a.region": "us-east-1"
    "spark.driver.extraJavaOptions": "-Dcom.amazonaws.services.s3.enableV4 -Daws.region=us-east-1 -Daws.accessKeyId=admin -Daws.secretAccessKey=Qwerty@1234"
    "spark.executor.extraJavaOptions": "-Dcom.amazonaws.services.s3.enableV4 -Daws.region=us-east-1 -Daws.accessKeyId=admin -Daws.secretAccessKey=Qwerty@1234"
    "spark.archives": "s3a://jobs/src.zip#jobs"
    "spark.sql.catalog.raw.uri": "jdbc:postgresql://host.docker.internal:5432/datalake"
    "spark.sql.catalog.raw.jdbc.user": "postgres"
    "spark.sql.catalog.raw.jdbc.password": "admin"
    "spark.sql.catalog.golden.uri": "jdbc:postgresql://host.docker.internal:5432/datalake"
    "spark.sql.catalog.golden.jdbc.user": "postgres"
    "spark.sql.catalog.golden.jdbc.password": "admin"
  restartPolicy:
    type: Never
  driver:
    cores: 1
    memory: 512m
    serviceAccount: spark-operator-spark
  executor:
    instances: 1
    cores: 1
    memory: 512m